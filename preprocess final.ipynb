{"cells":[{"cell_type":"markdown","metadata":{"id":"6bvlYaayYuCI"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6252,"status":"ok","timestamp":1702983377091,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"N2R7JQdHFGBz"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys\n","from imblearn.over_sampling import RandomOverSampler\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702983377091,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"IwmbSKjrFF_c"},"outputs":[],"source":["# train_path = \"trainingaugnew.csv\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3876,"status":"ok","timestamp":1702983380964,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"qMlPQF13FF9P"},"outputs":[],"source":["train_df = pd.read_csv('trainnew.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"EFbdl2AUYuCO","outputId":"e4476f55-7276-4482-9c98-bd88de440080"},"outputs":[],"source":["\n","classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","\n","def updatedf(dfold):\n","\n","    # Add new columns with initial value 0\n","    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n","\n","    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n","    for index, row in dfold.iterrows():\n","        if row['Label1'] in classes:\n","            dfold.at[index, row['Label1']] = 1\n","        if row['Label2'] in classes:\n","            dfold.at[index, row['Label2']] = 1\n","        if row['Label3'] in classes:\n","            dfold.at[index, row['Label3']] = 1\n","\n","    # Print the updated DataFrame\n","    print(dfold)\n","    return dfold\n","\n","\n","train_df=updatedf(train_df)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"XY8o-zpmFF1z"},"outputs":[],"source":["# dropping useless features/columns\n","train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"5nY-51AnIFd5","outputId":"fb0fe19a-444f-4ea5-a3f1-72c549e63094"},"outputs":[],"source":["train_df.columns"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702983381527,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"nkRnczi6G0ck"},"outputs":[],"source":["# rearranging columns\n","train_df = train_df[['Tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy',\n","       'political', 'country', 'rushed', 'ingredients', 'side-effect',\n","       'ineffective', 'religious', 'none']]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_df_random=train_df"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1702977963848,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"UkrExSqJY3ma"},"outputs":[],"source":["target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc_8AadgYuCZ","outputId":"d78abec1-d692-4f33-8cb2-424fe10ea8af"},"outputs":[],"source":["def gentrain_df(train_df,train_df_random,classname,count):\n","    religious_rows = train_df[train_df[classname] == 1]\n","\n","    from transformers import pipeline\n","    import pandas as pd\n","\n","    # Define a function to generate text using ChatGPT\n","    def generate_text(prompt):\n","        generator = pipeline(\"text-generation\", model=\"gpt2\")\n","        return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","    target_row_count = count\n","\n","    # Create an empty DataFrame to store the results\n","    generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","    generated_rows_notgpt = pd.DataFrame(columns=train_df.columns) #generating a random generation of same tweets without chatgpt\n","\n","    # Iterate through the 'religious_rows' and generate new rows\n","    for index, row in religious_rows.iterrows():\n","        tweet = row['Tweet']\n","        label_values = row.drop(['Tweet'])  # Remove the 'Tweet' column\n","        generated_tweet = generate_text(tweet)\n","        print('TWEET',tweet)\n","        print('GENERATED',generated_tweet)\n","        # print(label_values)\n","        new_row = pd.DataFrame({\n","            'Tweet': [generated_tweet],\n","            **label_values  # Copy the label values from the original row\n","        })\n","        # print(new_row)\n","        new_row_raw = pd.DataFrame({\n","            'Tweet': [generated_tweet],\n","            **label_values  # Copy the label values from the original row\n","        })\n","        # print(new_row_raw)\n","        generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","        generated_rows_notgpt= pd.concat([generated_rows_notgpt, new_row_raw], ignore_index=True)\n","        if len(generated_rows) >= target_row_count:\n","            break\n","\n","    train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","    train_df_random=pd.concat([train_df_random, generated_rows_notgpt], ignore_index=True)\n","    return train_df, train_df_random\n","\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'religious',1)\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'country',1)\n","train_df,train_df_random=gentrain_df(train_df,train_df_random,'ingredients',10)\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'conspiracy',10)\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'political',1)\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'none',1)\n","\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'unnecessary',1)\n","# train_df,train_df_random=gentrain_df(train_df,train_df_random,'mandatory',1)\n","\n","\n","\n","\n","# target_list = ['unnecessary', '', 'pharma', '', '', '', 'rushed', '', 'side-effect', 'ineffective', '', 'none']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["religious_rows = train_df[train_df['none'] == 1]\n","\n","from transformers import pipeline\n","import pandas as pd\n","\n","# Define a function to generate text using ChatGPT\n","def generate_text(prompt):\n","    generator = pipeline(\"text-generation\", model=\"gpt2\")\n","    return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","# Set the desired target row count (150 in your case)\n","target_row_count = 700\n","\n","# Create an empty DataFrame to store the results\n","generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","# Iterate through the 'religious_rows' and generate new rows\n","for index, row in religious_rows.iterrows():\n","    tweet = row['Tweet']\n","    label_values = row.drop(['Tweet'])  # Remove the 'Tweet' column\n","    generated_tweet = generate_text(tweet)\n","\n","    new_row = pd.DataFrame({\n","        'Tweet': [generated_tweet],\n","        **label_values  # Copy the label values from the original row\n","    })\n","\n","    generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","    # Break the loop if we reach the target row count\n","    if len(generated_rows) >= target_row_count:\n","        break\n","\n","# # Ensure the final DataFrame has the desired number of rows\n","# generated_rows = generated_rows.head(target_row_count)\n","train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","# # 'generated_rows' now contains 150 rows with generated \"Tweet\" and the 12 columns of labels\n","print(\"1\")\n","\n","\n","\n","\n","religious_rows = train_df[train_df['unnecessary'] == 1]\n","\n","from transformers import pipeline\n","import pandas as pd\n","\n","# Define a function to generate text using ChatGPT\n","def generate_text(prompt):\n","    generator = pipeline(\"text-generation\", model=\"gpt2\")\n","    return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","# Set the desired target row count (150 in your case)\n","target_row_count = 200\n","\n","# Create an empty DataFrame to store the results\n","generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","# Iterate through the 'religious_rows' and generate new rows\n","for index, row in religious_rows.iterrows():\n","    tweet = row['Tweet']\n","    label_values = row.drop(['Tweet'])  # Remove the 'Tweet' column\n","    generated_tweet = generate_text(tweet)\n","\n","    new_row = pd.DataFrame({\n","        'Tweet': [generated_tweet],\n","        **label_values  # Copy the label values from the original row\n","    })\n","\n","    generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","    # Break the loop if we reach the target row count\n","    if len(generated_rows) >= target_row_count:\n","        break\n","\n","# # Ensure the final DataFrame has the desired number of rows\n","# generated_rows = generated_rows.head(target_row_count)\n","train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","# # 'generated_rows' now contains 150 rows with generated \"Tweet\" and the 12 columns of labels\n","print(\"2\")\n","\n","religious_rows = train_df[train_df['ineffective'] == 1]\n","\n","from transformers import pipeline\n","import pandas as pd\n","\n","# Define a function to generate text using ChatGPT\n","def generate_text(prompt):\n","    generator = pipeline(\"text-generation\", model=\"gpt2\")\n","    return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","# Set the desired target row count (150 in your case)\n","target_row_count = 400\n","\n","# Create an empty DataFrame to store the results\n","generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","# Iterate through the 'religious_rows' and generate new rows\n","for index, row in religious_rows.iterrows():\n","    tweet = row['Tweet']\n","    label_values = row.drop(['Tweet'])  # Remove the 'Tweet' column\n","    generated_tweet = generate_text(tweet)\n","\n","    new_row = pd.DataFrame({\n","        'Tweet': [generated_tweet],\n","        **label_values  # Copy the label values from the original row\n","    })\n","\n","    generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","    # Break the loop if we reach the target row count\n","    if len(generated_rows) >= target_row_count:\n","        break\n","\n","# # Ensure the final DataFrame has the desired number of rows\n","# generated_rows = generated_rows.head(target_row_count)\n","train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","# # 'generated_rows' now contains 150 rows with generated \"Tweet\" and the 12 columns of labels\n","\n","print(\"3\")\n","\n","religious_rows = train_df[train_df['rushed'] == 1]\n","\n","from transformers import pipeline\n","import pandas as pd\n","\n","# Define a function to generate text using ChatGPT\n","def generate_text(prompt):\n","    generator = pipeline(\"text-generation\", model=\"gpt2\")\n","    return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","# Set the desired target row count (150 in your case)\n","target_row_count = 200\n","\n","# Create an empty DataFrame to store the results\n","generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","# Iterate through the 'religious_rows' and generate new rows\n","for index, row in religious_rows.iterrows():\n","    tweet = row['Tweet']\n","    label_values = row.drop(['Tweet'])  # Remove the 'Tweet' column\n","    generated_tweet = generate_text(tweet)\n","\n","    new_row = pd.DataFrame({\n","        'Tweet': [generated_tweet],\n","        **label_values  # Copy the label values from the original row\n","    })\n","\n","    generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","    # Break the loop if we reach the target row count\n","    if len(generated_rows) >= target_row_count:\n","        break\n","\n","# # Ensure the final DataFrame has the desired number of rows\n","# generated_rows = generated_rows.head(target_row_count)\n","train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","# # 'generated_rows' now contains 150 rows with generated \"Tweet\" and the 12 columns of labels\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2_Ru79EYuCd"},"outputs":[],"source":["# train_df.to_csv('oversampledtrain.csv', index=False) #before reviewer comment\n","\n","train_df_random.to_csv('trainRandomOversample.csv', index=False)\n","train_df.to_csv('trainGptOversample.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"V-KpcO69YuCg"},"source":["# Classes Count"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","# train_df = pd.read_csv('trainnew.csv')\n","target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","train_df.head\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# Assuming 'target_list' and 'train_df' are defined\n","\n","full_counts = {}\n","fsum = 0\n","\n","for column in target_list:\n","    count = sum(train_df[column])\n","    full_counts[column] = count\n","    fsum += count\n","\n","# Calculate the average count\n","avg_count = fsum / len(target_list)\n","\n","# Create a DataFrame for clarity\n","counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n","counts.index.name = 'label'\n","\n","def calculate_imbalance_ratio(count):\n","    # Correctly calculate IR for each class using majority class count\n","    majority_count = counts['full_count'].max()\n","    imbalance_ratio =np.round( majority_count / count)\n","    return imbalance_ratio\n","\n","counts['imbalance_ratio'] = counts['full_count'].apply(calculate_imbalance_ratio)\n","\n","print(counts)  # Display results in a more readable format\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1702978039731,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"xFYKmobzYuCg","outputId":"7539e8df-f5ac-446d-b946-740a1e95eb49"},"outputs":[],"source":["import pandas as pd\n","# train_df = pd.read_csv('trainnew.csv')\n","target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","train_df.head\n","# Assuming 'target_list' and 'train_df' are defined\n","import numpy as np\n","full_counts = {}\n","fsum = 0\n","\n","for column in target_list:\n","    # full_counts = {}\n","\n","    count = sum(train_df[column])\n","    full_counts[column] = count\n","    fsum += count\n","\n","full_counts['avg'] = int(fsum / len(target_list))\n","\n","counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n","counts.index.name = 'label'\n","print(full_counts)\n","\n","def set_sample_ratio(x):\n","    avg = int(counts['full_count'].loc['avg'])\n","    x = int(x)\n","    if x >= avg: return 1\n","    else: return int(np.round(avg / x))\n","\n","counts['calculated_oversampling_ratio'] = counts['full_count'].apply(set_sample_ratio)\n","counts.T"]},{"cell_type":"markdown","metadata":{},"source":["CORRECT IMBALANCE RATIO"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from transformers import pipeline\n","import pandas as pd\n","labels =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","\n","# ... (existing code) ...\n","\n","# Calculate imbalance ratios for all classes\n","# imbalance_ratios = calculate_imbalance_ratio(counts)\n","# Iterate through imbalanced classes and generate entries\n","for i, (label, count) in enumerate(zip(labels, counts)):\n","    print(counts['imbalance_ratio'][i]  )\n","    if counts['imbalance_ratio'][i] > 1:\n","        class_rows = train_df[train_df[label] == 1]\n","        generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","        while imbalance_ratios[i] > 1:\n","            for index, row in class_rows.iterrows():\n","                tweet = row['Tweet']\n","                label_values = row.drop(['Tweet'])\n","                generated_tweet = generate_text(tweet)\n","\n","                new_row = pd.DataFrame({\n","                    'Tweet': [generated_tweet],\n","                    **label_values\n","                })\n","\n","                generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","\n","            # Recalculate imbalance ratios after adding generated entries\n","            train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","            counts = counts.add(generated_rows['full_count'], fill_value=0)  # Update counts\n","            imbalance_ratios = calculate_imbalance_ratio(counts)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import numpy as np  # This line is redundant; remove it.\n","from transformers import pipeline\n","import pandas as pd\n","\n","# Assuming 'target_list' and 'train_df' are defined\n","\n","full_counts = {}\n","fsum = 0\n","\n","for column in target_list:\n","    count = sum(train_df[column].values)  # Use .values to convert Series to array\n","    full_counts[column] = count\n","    fsum += count\n","\n","# Calculate the average count\n","avg_count = fsum / len(target_list)\n","\n","# Create a DataFrame for clarity\n","counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n","counts.index.name = 'label'\n","\n","def calculate_imbalance_ratio(count):\n","    # Correctly calculate IR for each class using majority class count\n","    majority_count = counts['full_count'].max()\n","    imbalance_ratio = majority_count / count\n","    return imbalance_ratio\n","\n","counts['imbalance_ratio'] = counts['full_count'].apply(calculate_imbalance_ratio)\n","\n","# ... (existing code) ...\n","\n","# Calculate imbalance ratios for all classes\n","# imbalance_ratios = calculate_imbalance_ratio(counts)\n","def generate_text(prompt):\n","    generator = pipeline(\"text-generation\", model=\"gpt2\")\n","    return generator(prompt, max_length=128, num_return_sequences=1)[0]['generated_text']\n","\n","\n","labels = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","\n","# ... (existing code) ...\n","\n","# Calculate imbalance ratios for all classes\n","imbalance_ratios = []\n","for _, count in zip(labels, counts['full_count']):\n","    imbalance_ratios.append(calculate_imbalance_ratio(count))\n","\n","# Iterate through imbalanced classes and generate entries\n","for i, (label, count) in enumerate(zip(labels, counts)):\n","    print(counts['imbalance_ratio'][i]  )\n","    if counts['imbalance_ratio'][i] > 1:\n","        class_rows = train_df[train_df[label] == 1]\n","        generated_rows = pd.DataFrame(columns=train_df.columns)\n","\n","        while imbalance_ratios[i] > 1:\n","            for index, row in class_rows.iterrows():\n","                tweet = row['Tweet']\n","                label_values = row.drop(['Tweet'])\n","                generated_tweet = generate_text(tweet)\n","\n","                new_row = pd.DataFrame({\n","                    'Tweet': [generated_tweet],\n","                    **label_values\n","                })\n","\n","                generated_rows = pd.concat([generated_rows, new_row], ignore_index=True)\n","\n","            # Recalculate imbalance ratios after adding generated entries\n","            train_df = pd.concat([train_df, generated_rows], ignore_index=True)\n","            full_counts = {}\n","            fsum = 0\n","\n","            for column in target_list:\n","                count = sum(train_df[column].values)  # Use .values to convert Series to array\n","                full_counts[column] = count\n","                fsum += count\n","\n","            counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n","            counts.index.name = 'label'\n","\n","            counts['imbalance_ratio'] = counts['full_count'].apply(calculate_imbalance_ratio)\n","\n","            counts = counts.add(generated_rows['full_count'], fill_value=0)  # Update counts\n","            print(counts)\n","            imbalance_ratios[i] = calculate_imbalance_ratio(counts['full_count'][label])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df.to_csv('oversampled.csv', index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["tdn0Bhs-YuCR","BFz9iR6hYuCU","WipbuCXBYuCX"],"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
