{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pegasus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('pegasus.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path,ID):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      predicted_values,predicted_raw=LetsAssess(model)\n",
        "      np.save('predicted_raw_epoch'+str(epoch-1)+str(ID)+'.npy', predicted_raw)\n",
        "\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'BU')\n",
        "predicted_valuesBU,predicted_rawBU=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelB.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'seantw')\n",
        "predicted_valuesSN,predicted_rawSN=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'DL')\n",
        "predicted_valuesDL,predicted_rawDL=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('rawoversample.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qSjmKyYJIl2M"
      },
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "YMpQi7MpRPVb"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2YXXbsj0Iltp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRUtQwS5Snqa"
      },
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ye-qQhzCWglF"
      },
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "R08BB9adUNI4"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path,ID):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      predicted_values,predicted_raw=LetsAssess(model)\n",
        "      np.save('predicted_raw_epoch'+str(epoch-1)+str(ID)+'.npy', predicted_raw)\n",
        "\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'BU')\n",
        "predicted_valuesBU,predicted_rawBU=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelB.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'seantw')\n",
        "predicted_valuesSN,predicted_rawSN=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'DL')\n",
        "predicted_valuesDL,predicted_rawDL=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('trainGptOversample.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path,ID):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      predicted_values,predicted_raw=LetsAssess(model)\n",
        "      np.save('predicted_raw_epoch'+str(epoch-1)+str(ID)+'.npy', predicted_raw)\n",
        "\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'BU')\n",
        "predicted_valuesBU,predicted_rawBU=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelB.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'seantw')\n",
        "predicted_valuesSN,predicted_rawSN=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'DL')\n",
        "predicted_valuesDL,predicted_rawDL=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('train_BART.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path,ID):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      predicted_values,predicted_raw=LetsAssess(model)\n",
        "      np.save('predicted_raw_epoch'+str(epoch-1)+str(ID)+'.npy', predicted_raw)\n",
        "\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'BU')\n",
        "predicted_valuesBU,predicted_rawBU=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelB.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'seantw')\n",
        "predicted_valuesSN,predicted_rawSN=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'DL')\n",
        "predicted_valuesDL,predicted_rawDL=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "import torch.nn.functional as F\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('rawoversample.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path,ID):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      predicted_values,predicted_raw=LetsAssess(model)\n",
        "      np.save('predicted_raw_epoch'+str(epoch-1)+str(ID)+'.npy', predicted_raw)\n",
        "\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'BU')\n",
        "predicted_valuesBU,predicted_rawBU=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "# tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        # self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelB.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'seantw')\n",
        "predicted_valuesSN,predicted_rawSN=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "# tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()    \n",
        "\n",
        "        # self.bert = BertModel.from_pretrained('bert-large-uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path,'DL')\n",
        "predicted_valuesDL,predicted_rawDL=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "146f6cea72b94d27a404c152d7c49716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee6c862f48c482981afdc1e4a0a16ae",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee07da1b2452472d99491785bd44c66a",
            "value": 28
          }
        },
        "1ee6c862f48c482981afdc1e4a0a16ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0901642f2745a7bc6da433de120c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38bfcfc2b4fc4972a20b5d1d45317243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "421525a424424b6e8cabc4c2124339c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4994f208620c4663b930f3690b307231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edfa42ca0214af4afa918898465d939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a58a3dded7d4649a53dc4c02b426cc4",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db1b7ef2a88b475c91f5fec00ef36c6c",
            "value": 466062
          }
        },
        "5c7fda1ef17d421ba20c721f640e6192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73e15358fbc34623977471e113035b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4994f208620c4663b930f3690b307231",
            "placeholder": "",
            "style": "IPY_MODEL_5c7fda1ef17d421ba20c721f640e6192",
            "value": " 28.0/28.0 [00:01&lt;00:00, 27.8B/s]"
          }
        },
        "7d613ba2dfc84ec2bd83a834f8053ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7f4a347af12f404bb6bd11a0be2cc8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_146f6cea72b94d27a404c152d7c49716",
              "IPY_MODEL_73e15358fbc34623977471e113035b8f"
            ],
            "layout": "IPY_MODEL_38bfcfc2b4fc4972a20b5d1d45317243"
          }
        },
        "80c47bb21c02403ab01e1a87343f52b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831ca81d5e28459d8158e1fcd16cf112",
            "placeholder": "",
            "style": "IPY_MODEL_421525a424424b6e8cabc4c2124339c5",
            "value": " 466k/466k [00:00&lt;00:00, 2.26MB/s]"
          }
        },
        "831ca81d5e28459d8158e1fcd16cf112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a58a3dded7d4649a53dc4c02b426cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9196e60b61954fb78571c123e780a4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada2275d1c4f43ad9c04e6f702ce8ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b799caea4b8c434ebbb86886a5fa7dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc26fb52ff7f400291cc9fdcf9f49e86",
              "IPY_MODEL_f84eb759ba124713b2736fa6e4c308e6"
            ],
            "layout": "IPY_MODEL_bc1eaff9c68e4e98bbd3a373dc1f4e0b"
          }
        },
        "ba5e219b14cf429faf2e6f97b811b327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4edfa42ca0214af4afa918898465d939",
              "IPY_MODEL_80c47bb21c02403ab01e1a87343f52b6"
            ],
            "layout": "IPY_MODEL_2c0901642f2745a7bc6da433de120c8e"
          }
        },
        "bc1eaff9c68e4e98bbd3a373dc1f4e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc26fb52ff7f400291cc9fdcf9f49e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ada2275d1c4f43ad9c04e6f702ce8ebb",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d613ba2dfc84ec2bd83a834f8053ea0",
            "value": 231508
          }
        },
        "cea1b70019b1421a9c67285dcdcbbe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db1b7ef2a88b475c91f5fec00ef36c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "ee07da1b2452472d99491785bd44c66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f84eb759ba124713b2736fa6e4c308e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9196e60b61954fb78571c123e780a4bf",
            "placeholder": "",
            "style": "IPY_MODEL_cea1b70019b1421a9c67285dcdcbbe4a",
            "value": " 232k/232k [00:02&lt;00:00, 99.2kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
