{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('rawoversample.csv', encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "  batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "def rephrase_tweets(train_df, num_return_sequences=4, num_beams=4):\n",
        "    \"\"\"Rephrases tweets in the 'Tweet' column of a DataFrame using Pegasus.\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): The DataFrame containing the tweets.\n",
        "        num_return_sequences (int): The number of generated sequences.\n",
        "        num_beams (int): The number of beams for beam search.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the rephrased tweets in a new column.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = 'tuner007/pegasus_paraphrase'\n",
        "    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "    rephrased_tweets = []\n",
        "    for tweet in train_df['Tweet']:\n",
        "        batch = tokenizer([tweet], truncation=True, padding='longest', max_length=120, return_tensors=\"pt\").to(torch_device)\n",
        "        translated = model.generate(**batch, max_length=120, num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "        tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "        rephrased_tweets.append(tgt_text)\n",
        "\n",
        "    train_df['Rephrased_Tweet'] = rephrased_tweets\n",
        "    return train_df\n",
        "\n",
        "# Example usage:\n",
        "# train_df = pd.read_csv('your_data.csv')\n",
        "rephrased_df = rephrase_tweets(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "rephrased_df.to_csv('pegasus.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('rawoversample.csv', encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "def rephrase_tweets(train_df, num_return_sequences=4, num_beams=4):\n",
        "    \"\"\"Rephrases tweets in the 'Tweet' column of a DataFrame using BART.\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): The DataFrame containing the tweets.\n",
        "        num_return_sequences (int): The number of generated sequences.\n",
        "        num_beams (int): The number of beams for beam search.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the rephrased tweets in a new column.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = 'eugenesiow/bart-paraphrase'  # Replace with your desired BART model\n",
        "    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "    rephrased_tweets = []\n",
        "    for tweet in train_df['Tweet']:\n",
        "        batch = tokenizer([tweet], truncation=True, padding='longest', max_length=120, return_tensors=\"pt\").to(torch_device)\n",
        "        translated = model.generate(**batch, max_length=120, num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "        tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "        rephrased_tweets.append(tgt_text)\n",
        "\n",
        "    train_df['Rephrased_Tweet'] = rephrased_tweets\n",
        "    return train_df\n",
        "\n",
        "# Example usage:\n",
        "# train_df = pd.read_csv('your_data.csv')\n",
        "rephrased_df = rephrase_tweets(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_df.to_csv('train_BART.csv', index=False)\n",
        "rephrased_df.to_csv('train_BART.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_path = \"trainnew.csv\"\n",
        "train_df = pd.read_csv('pegasus.csv', encoding='utf-8')\n",
        "\n",
        "val_path=\"valnew.csv\"\n",
        "test_path = \"testnew.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "test_df = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "# train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dropping useless features/columns\n",
        "# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rearranging columns\n",
        "train_df = train_df[['Tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy',\n",
        "       'political', 'country', 'rushed', 'ingredients', 'side-effect',\n",
        "       'ineffective', 'religious', 'none']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping\n",
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05\n",
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n",
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "\n",
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "# tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      LetsAssess(model)\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransModel2(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[16, 32, 64, 128]):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransModel2()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n",
        "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesD,predicted_rawD=LetsAssess(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'trainRandomOversample.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Load the text generation pipeline for rephrasing\n",
        "rephraser = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "\n",
        "# Define a function to rephrase the tweets using the model\n",
        "def advanced_rephrase(tweet):\n",
        "    try:\n",
        "        rephrased = rephraser(f\"paraphrase: {tweet} </s>\", max_length=128, do_sample=False)\n",
        "        return rephrased[0]['generated_text']\n",
        "    except:\n",
        "        return tweet\n",
        "\n",
        "# Apply the rephrasing function to the 'Tweet' column\n",
        "df['Rephrased_Tweet'] = df['Tweet'].apply(advanced_rephrase)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('paraph-t5-base.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df['Tweet'][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransModel3(nn.Module):\n",
        "    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransModel3()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesE,predicted_rawE=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel4(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')  \n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel4()\n",
        "model.to(device)\n",
        "print(model)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesG,predicted_rawG=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "predicted_values=np.zeros((test_df.shape[0],12))\n",
        "# pv=(predicted_rawB+predicted_rawD+predicted_rawC+predicted_rawF+predicted_rawG)/6#+predicted_rawH)/7\n",
        "# pv=(predicted_rawC+predicted_rawD)/2\n",
        "# pv=(predicted_rawB+predicted_rawC+predicted_rawD+predicted_rawE+predicted_rawF)/5#+predicted_rawH)/7\n",
        "\n",
        "# pv=(predicted_rawB+predicted_rawW)/2\n",
        "\n",
        "# pv=(predicted_rawG+predicted_rawB+predicted_rawC)/3\n",
        "pv=predicted_rawE\n",
        "for i,text in enumerate(test_df['Tweet']):    \n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "    predictions = np.zeros(probs.shape)\n",
        "    predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "    if(sum(predictions))==0:\n",
        "         argmax_index = probs.argmax()\n",
        "         predictions[argmax_index] = 1\n",
        "    predicted_values[i]=predictions\n",
        "\n",
        "    \n",
        "print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "# Calculate Jaccard score for each sample individually\n",
        "sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "# Calculate the average Jaccard score\n",
        "average_jaccard = np.mean(sample_jaccard_scores)\n",
        "print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import numpy as np\n",
        "# 'A1':predicted_rawA,\n",
        "arrays = {'B1': predicted_rawB,'C1': predicted_rawC,'D1': predicted_rawD,'E1': predicted_rawE, 'F1':predicted_rawF,'G1': predicted_rawG,'W1': predicted_rawW}#, 'H1':predicted_rawH, 'T1':predicted_rawT,'R1':predicted_rawR}\n",
        "best_score = 0\n",
        "best_combination = None\n",
        "\n",
        "for r in range(1, len(arrays) + 1):\n",
        "    for combination in combinations(arrays.items(), r):\n",
        "        # avg = sum(array for name, array in combination) / len(combination)\n",
        "        avg = np.mean([array for name, array in combination], axis=0)\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(avg).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        for i in range(predictions.shape[0]):\n",
        "            if np.all(predictions[i] == 0):\n",
        "                argmax_index = probs[i].argmax()\n",
        "                predictions[i][argmax_index] = 1\n",
        "        \n",
        "        report = classification_report(y_true, predicted_values, output_dict=True)\n",
        "        # score = report['macro avg']['f1-score']  # Replace with your preferred metric\n",
        "        score = accuracy_score(y_true, predictions)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_combination = [name for name, array in combination]\n",
        "\n",
        "        # print(\"Combination:\", [name for name, array in combination])\n",
        "\n",
        "print(\"Best score:\", best_score)\n",
        "print(\"Best combination:\", best_combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AdamW\n",
        "\n",
        "##Loading processed data\n",
        "\n",
        "# train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n",
        "# train_df2 = pd.read_csv('trainaug.csv')\n",
        "train_df=pd.read_csv('trainnew.csv', encoding='utf-8')\n",
        "# train_df= pd.concat([train_df, train_df2], ignore_index=True)\n",
        "val_df = pd.read_csv('valnew.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('testnew.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "\n",
        "def updatedf(dfold):\n",
        "\n",
        "    # Add new columns with initial value 0\n",
        "    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n",
        "\n",
        "    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n",
        "    for index, row in dfold.iterrows():\n",
        "        if row['Label1'] in classes:\n",
        "            dfold.at[index, row['Label1']] = 1\n",
        "        if row['Label2'] in classes:\n",
        "            dfold.at[index, row['Label2']] = 1\n",
        "        if row['Label3'] in classes:\n",
        "            dfold.at[index, row['Label3']] = 1\n",
        "\n",
        "    # Print the updated DataFrame\n",
        "    print(dfold)\n",
        "    return dfold\n",
        "\n",
        "\n",
        "train_df=updatedf(train_df)\n",
        "val_df=updatedf(val_df)\n",
        "test_df=updatedf(test_df)\n",
        "# dropping useless features/columns\n",
        "train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n",
        "test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSjmKyYJIl2M"
      },
      "outputs": [],
      "source": [
        "target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n",
        "# hyperparameters\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'target_list' and 'train_df' are defined\n",
        "import numpy as np\n",
        "full_counts = {}\n",
        "fsum = 0\n",
        "\n",
        "for column in target_list:\n",
        "    # full_counts = {}\n",
        "    \n",
        "    count = sum(train_df[column])\n",
        "    full_counts[column] = count\n",
        "    fsum += count\n",
        "\n",
        "full_counts['avg'] = int(fsum / len(target_list))\n",
        "\n",
        "counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n",
        "counts.index.name = 'label'\n",
        "print(full_counts)\n",
        "\n",
        "def set_sample_ratio(x):\n",
        "    avg = int(counts['full_count'].loc['avg'])\n",
        "    x = int(x)\n",
        "    if x >= avg: return 1\n",
        "    else: return int(np.round(avg / x))\n",
        "\n",
        "counts['calculated_oversampling_ratio'] = counts['full_count'].apply(set_sample_ratio)\n",
        "counts.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X=train_df['Tweet']\n",
        "y=train_df[target_list]\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn scikit-multilearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# NLP\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re\n",
        "\n",
        "# Warning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_and_clean(text):\n",
        "    # Changing case of the text to lower case\n",
        "    lowered = text.lower()\n",
        "    \n",
        "    # Cleaning the text\n",
        "    cleaned = re.sub('@user', '', lowered)\n",
        "    \n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(cleaned)\n",
        "    filtered_tokens = [token for token in tokens if re.match(r'\\w{1,}', token)]\n",
        "    \n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "# Ensure the tokenizer function is compatible with TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_clean, stop_words='english', token_pattern=None)\n",
        "\n",
        "# Example DataFrames (replace with your actual data)\n",
        "import pandas as pd\n",
        "# train_df = pd.DataFrame({'Tweet': [\"I love this!\", \"This is terrible.\", \"Feeling great today!\"]})\n",
        "# test_df = pd.DataFrame({'Tweet': [\"What a wonderful day!\", \"I hate this.\", \"So happy right now!\"]})\n",
        "\n",
        "# Transform the data\n",
        "X_train_tweets_tfidf = tfidf_vectorizer.fit_transform(train_df['Tweet'])\n",
        "X_test_tweets_tfidf = tfidf_vectorizer.transform(test_df['Tweet'])\n",
        "\n",
        "print(X_train_tweets_tfidf.shape, X_test_tweets_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Imbalance Check\n",
        "y_train=train_df['conspiracy']\n",
        "plt.pie(train_df['conspiracy'].value_counts(), \n",
        "        labels=['Label 0 (Positive Tweets)', 'Label 1 (Negative Tweets)'], \n",
        "        autopct='%0.1f%%')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(np.shape(X_train_tweets_tfidf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "other_columns = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'ineffective', 'religious', 'none']\n",
        "\n",
        "side_effect_rows = train_df[(train_df['side-effect'] == 1) & (train_df[other_columns].sum(axis=1) == 0)]\n",
        "rows_to_remove = side_effect_rows.sample(frac=1/3, random_state=1)\n",
        "print(rows_to_remove.index)\n",
        "train_df = train_df.drop(rows_to_remove.index)\n",
        "train_df = train_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'target_list' and 'train_df' are defined\n",
        "import numpy as np\n",
        "full_counts = {}\n",
        "fsum = 0\n",
        "\n",
        "for column in target_list:\n",
        "    # full_counts = {}\n",
        "    \n",
        "    count = sum(train_df[column])\n",
        "    full_counts[column] = count\n",
        "    fsum += count\n",
        "\n",
        "full_counts['avg'] = int(fsum / len(target_list))\n",
        "\n",
        "counts = pd.DataFrame.from_dict(full_counts, orient='index', columns=['full_count'])\n",
        "counts.index.name = 'label'\n",
        "print(full_counts)\n",
        "\n",
        "def set_sample_ratio(x):\n",
        "    avg = int(counts['full_count'].loc['avg'])\n",
        "    x = int(x)\n",
        "    if x >= avg: return 1\n",
        "    else: return int(np.round(avg / x))\n",
        "\n",
        "counts['calculated_oversampling_ratio'] = counts['full_count'].apply(set_sample_ratio)\n",
        "counts.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Common for Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMpQi7MpRPVb"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import shutil\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from early_stopping import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YXXbsj0Iltp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRUtQwS5Snqa"
      },
      "outputs": [],
      "source": [
        "import torch.cuda\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "!nvcc --version\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye-qQhzCWglF"
      },
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R08BB9adUNI4"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\"\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode(predicted_values,pv):\n",
        "    # predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    # pv=(predicted_rawA+predicted_rawB+predicted_rawC)/3\n",
        "    for i,text in enumerate(test_df['Tweet']):    \n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "        predicted_values[i]=predictions\n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "        \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LetsAssess(modelTBA):\n",
        "    id2label = {idx:label for idx, label in enumerate(target_list)}\n",
        "    label2id = {label:idx for idx, label in enumerate(target_list)}\n",
        "\n",
        "    #####Pass on all tweets and find their labels using the trained_model\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "\n",
        "    predicted_labels = []\n",
        "    predicted_single_labels=[]\n",
        "    predicted_values=np.zeros((test_df.shape[0],12))\n",
        "    predicted_raw=np.zeros((test_df.shape[0],12))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modelTBA.to(device)\n",
        "\n",
        "    for i,text in enumerate(test_df['Tweet']):\n",
        "        \n",
        "        encodings = tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Move the encodings to the device\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        # # Perform the forward pass\n",
        "        with torch.no_grad():\n",
        "            output = modelTBA(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # Apply sigmoid + threshold\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(output.squeeze().cpu())    \n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs >= 0.5)] = 1\n",
        "        if(sum(predictions))==0:\n",
        "            argmax_index = probs.argmax()\n",
        "            predictions[argmax_index] = 1\n",
        "            # predictions[np.where(probs >= 0.4)] = 1\n",
        "            # if(sum(predictions))==0:\n",
        "            #     predictions[np.where(probs >= 0.3)] = 1\n",
        "            #     if(sum(predictions))==0:\n",
        "            #         predictions[np.where(probs >= 0.2)] = 1\n",
        "            #         if(sum(predictions))==0:\n",
        "            #             predictions[np.where(probs >= 0.1)] = 1\n",
        "            \n",
        "\n",
        "        predicted_values[i]=predictions\n",
        "        predicted_raw[i]=output.squeeze().cpu()\n",
        "        # # Turn predicted id's into actual label names\n",
        "        # predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]    \n",
        "        # # Get the predicted label index\n",
        "        # predicted_label_index = int(np.argmax(probs, axis=0))\n",
        "\n",
        "      \n",
        "\n",
        "    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "    print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "    multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "    # Calculate Jaccard score for each sample individually\n",
        "    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "    # Calculate the average Jaccard score\n",
        "    average_jaccard = np.mean(sample_jaccard_scores)\n",
        "    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "    return predicted_values, predicted_raw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "val_targets=[]\n",
        "val_outputs=[]\n",
        "\n",
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "  valid_loss_min = np.Inf\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(tqdm(training_loader)):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "      # early_stopping(valid_loss, model)\n",
        "        \n",
        "       \n",
        "        # save checkpoint\n",
        "      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "      LetsAssess(model)\n",
        "      # decode(predicted_values, predicted_raw )\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "      # if early_stopping.early_stop:\n",
        "      #       print(\"Early stopping\")\n",
        "      #       break \n",
        "    # scheduler.step()\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  # model.load_state_dict(torch.load(checkpoint_path))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer=DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n",
        "# tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        self.bert =DistilBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        # self.bert = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')  \n",
        "        # self.bert = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1')  \n",
        "\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesR,predicted_rawR=LetsAssess(model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        # self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8), num_layers=1)\n",
        "        # self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        # self.dense1 = nn.Linear(sum(output_sizes)*31, 256)  # First dense layer\n",
        "        # self.dense2 = nn.Linear(256, 128)  # Second dense layer\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        # cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        # x = cls_hs[0]\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # x = self.transformer(x)\n",
        "        # x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        # parallel_outputs = [layer(x).squeeze(2) for layer in self.parallel_layers]\n",
        "        # x = torch.cat(parallel_outputs, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        # x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        # x = F.relu(self.dense2(x))  # Apply ReLU activation function after the second dense layer\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesR,predicted_rawR=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel2(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128]):\n",
        "        super().__init__()        \n",
        "        # self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0]\n",
        "        x = self.dropout1(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n",
        "        x = torch.cat(parallel_outputs, dim=1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        x = self.dropout3(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel2()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesA,predicted_rawA=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        # self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8), num_layers=1)\n",
        "        # self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        # self.dense1 = nn.Linear(sum(output_sizes)*31, 256)  # First dense layer\n",
        "        # self.dense2 = nn.Linear(256, 128)  # Second dense layer\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        # cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        # x = cls_hs[0]\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # x = self.transformer(x)\n",
        "        # x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        # parallel_outputs = [layer(x).squeeze(2) for layer in self.parallel_layers]\n",
        "        # x = torch.cat(parallel_outputs, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        # x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        # x = F.relu(self.dense2(x))  # Apply ReLU activation function after the second dense layer\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesB,predicted_rawB=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        # self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.norm = nn.BatchNorm1d(self.bert.config.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.clf2 = nn.Linear(self.hidden_size, 256)\n",
        "        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        x = self.norm(x)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        x = self.clf2(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(20, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesT,predicted_rawT=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel3(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8), num_layers=1)\n",
        "        # self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        # self.dense1 = nn.Linear(sum(output_sizes)*31, 256)  # First dense layer\n",
        "        # self.dense2 = nn.Linear(256, 128)  # Second dense layer\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        # cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        # x = cls_hs[0]\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # x = self.transformer(x)\n",
        "        # x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        # parallel_outputs = [layer(x).squeeze(2) for layer in self.parallel_layers]\n",
        "        # x = torch.cat(parallel_outputs, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        # x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        # x = F.relu(self.dense2(x))  # Apply ReLU activation function after the second dense layer\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel3()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesC,predicted_rawC=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel4(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('CovRelex-SE/CORD19-BERT')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0]\n",
        "        x = self.dropout1(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n",
        "        x = torch.cat(parallel_outputs, dim=1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        x = self.dropout3(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "    \n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel4()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesD,predicted_rawD=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel5(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8), num_layers=1)\n",
        "        # self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        # self.dense1 = nn.Linear(sum(output_sizes)*31, 256)  # First dense layer\n",
        "        # self.dense2 = nn.Linear(256, 128)  # Second dense layer\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        # cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        # x = cls_hs[0]\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # x = self.transformer(x)\n",
        "        # x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        # parallel_outputs = [layer(x).squeeze(2) for layer in self.parallel_layers]\n",
        "        # x = torch.cat(parallel_outputs, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        # x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        # x = F.relu(self.dense2(x))  # Apply ReLU activation function after the second dense layer\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel5()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesE,predicted_rawE=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel6(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0]\n",
        "        x = self.dropout1(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n",
        "        x = torch.cat(parallel_outputs, dim=1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        x = self.dropout3(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "    \n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel6()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesF,predicted_rawF=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
        "# tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel7(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128,256]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=8), num_layers=1)\n",
        "        # self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        # self.dense1 = nn.Linear(sum(output_sizes)*31, 256)  # First dense layer\n",
        "        # self.dense2 = nn.Linear(256, 128)  # Second dense layer\n",
        "        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n",
        "        \n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        # cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        # x = cls_hs[0]\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0][:, 0, :] \n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        # x = self.transformer(x)\n",
        "        # x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        # parallel_outputs = [layer(x).squeeze(2) for layer in self.parallel_layers]\n",
        "        # x = torch.cat(parallel_outputs, dim=1)\n",
        "        # x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        # x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        # x = F.relu(self.dense2(x))  # Apply ReLU activation function after the second dense layer\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel7()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesG,predicted_rawG=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "import torch.nn.functional as F\n",
        "class CovModel8(nn.Module):\n",
        "    def __init__(self, num_layers=4, output_sizes=[64,128]):\n",
        "        super().__init__()        \n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')  \n",
        "        # self.bert =DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n",
        "\n",
        "    def forward(self, inputs, mask, labels):\n",
        "        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n",
        "        x = cls_hs[0]\n",
        "        x = self.dropout1(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = x.transpose(1, 2)  # Transpose the last two dimensions\n",
        "        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n",
        "        x = torch.cat(parallel_outputs, dim=1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n",
        "        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n",
        "        x = self.dropout3(x)\n",
        "        x = self.clf(x)\n",
        "        return x\n",
        "    \n",
        "best_model_path = \"modelA.pt\"    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CovModel8()\n",
        "model.to(device)\n",
        "print(model)\n",
        "# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-6)\n",
        "\n",
        "model = train_model(5, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "predicted_valuesH,predicted_rawH=LetsAssess(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_values=np.zeros((test_df.shape[0],12))\n",
        "pv=(predicted_rawB+predicted_rawD+predicted_rawC+predicted_rawF+predicted_rawG)/6#+predicted_rawH)/7\n",
        "# pv=(predicted_rawC+predicted_rawD)/2\n",
        "# pv=(predicted_rawB+predicted_rawC+predicted_rawD+predicted_rawE+predicted_rawF)/5#+predicted_rawH)/7\n",
        "\n",
        "# pv=(predicted_rawD+predicted_rawF)/2\n",
        "# pv=(predicted_rawA+predicted_rawD+predicted_rawF)/2\n",
        "# pv=predicted_rawA\n",
        "for i,text in enumerate(test_df['Tweet']):    \n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n",
        "    predictions = np.zeros(probs.shape)\n",
        "    predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "    if(sum(predictions))==0:\n",
        "         argmax_index = probs.argmax()\n",
        "         predictions[argmax_index] = 1\n",
        "        # predictions[np.where(probs.cpu() >= 0.4)] = 1\n",
        "        # if(sum(predictions))==0:\n",
        "        #     predictions[np.where(probs.cpu() >= 0.3)] = 1\n",
        "        #     if(sum(predictions))==0:\n",
        "        #         predictions[np.where(probs.cpu() >= 0.2)] = 1\n",
        "        #         if(sum(predictions))==0:\n",
        "        #             predictions[np.where(probs.cpu() >= 0.1)] = 1\n",
        "        #         else:\n",
        "        #              argmax_index = probs.argmax()\n",
        "        #              predictions[argmax_index] = 1\n",
        "            \n",
        "    predicted_values[i]=predictions\n",
        "\n",
        "y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n",
        "    \n",
        "print(classification_report(y_true, predicted_values,target_names=target_list))\n",
        "\n",
        "print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n",
        "\n",
        "multilabel_confusion_matrix(y_true, predicted_values)\n",
        "\n",
        "# Calculate Jaccard score for each sample individually\n",
        "sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n",
        "\n",
        "# Calculate the average Jaccard score\n",
        "average_jaccard = np.mean(sample_jaccard_scores)\n",
        "print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# # Save the arrays to .npy files\n",
        "# np.save('predicted_rawA.npy', predicted_rawA)\n",
        "# np.save('predicted_rawB.npy', predicted_rawB)\n",
        "# np.save('predicted_rawC.npy', predicted_rawC)\n",
        "\n",
        "# Load the arrays from .npy files\n",
        "# predicted_rawA = np.load('predicted_rawA.npy')\n",
        "# predicted_rawB = np.load('predicted_rawB.npy')\n",
        "# predicted_rawC = np.load('predicted_rawC.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "arrays = {'A1':predicted_rawA,'B1': predicted_rawB,'C1': predicted_rawC,'D1': predicted_rawD,'E1': predicted_rawE, 'F1':predicted_rawF,'G1': predicted_rawG, 'H1':predicted_rawH, 'T1':predicted_rawT,'R1':predicted_rawR}\n",
        "best_score = 0\n",
        "best_combination = None\n",
        "\n",
        "for r in range(1, len(arrays) + 1):\n",
        "    for combination in combinations(arrays.items(), r):\n",
        "        # avg = sum(array for name, array in combination) / len(combination)\n",
        "        avg = np.mean([array for name, array in combination], axis=0)\n",
        "        sigmoid = torch.nn.Sigmoid()\n",
        "        probs = sigmoid(torch.from_numpy(avg).to(device))\n",
        "        predictions = np.zeros(probs.shape)\n",
        "        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n",
        "        for i in range(predictions.shape[0]):\n",
        "            if np.all(predictions[i] == 0):\n",
        "                argmax_index = probs[i].argmax()\n",
        "                predictions[i][argmax_index] = 1\n",
        "        \n",
        "        # report = classification_report(y_true, predicted_values, output_dict=True)\n",
        "        # score = report['weighted avg']['f1-score']  # Replace with your preferred metric\n",
        "        score = accuracy_score(y_true, predictions)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_combination = [name for name, array in combination]\n",
        "\n",
        "        # print(\"Combination:\", [name for name, array in combination])\n",
        "\n",
        "print(\"Best score:\", best_score)\n",
        "print(\"Best combination:\", best_combination)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert-pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "146f6cea72b94d27a404c152d7c49716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee6c862f48c482981afdc1e4a0a16ae",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee07da1b2452472d99491785bd44c66a",
            "value": 28
          }
        },
        "1ee6c862f48c482981afdc1e4a0a16ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c0901642f2745a7bc6da433de120c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38bfcfc2b4fc4972a20b5d1d45317243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "421525a424424b6e8cabc4c2124339c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4994f208620c4663b930f3690b307231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edfa42ca0214af4afa918898465d939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a58a3dded7d4649a53dc4c02b426cc4",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db1b7ef2a88b475c91f5fec00ef36c6c",
            "value": 466062
          }
        },
        "5c7fda1ef17d421ba20c721f640e6192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73e15358fbc34623977471e113035b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4994f208620c4663b930f3690b307231",
            "placeholder": "​",
            "style": "IPY_MODEL_5c7fda1ef17d421ba20c721f640e6192",
            "value": " 28.0/28.0 [00:01&lt;00:00, 27.8B/s]"
          }
        },
        "7d613ba2dfc84ec2bd83a834f8053ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7f4a347af12f404bb6bd11a0be2cc8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_146f6cea72b94d27a404c152d7c49716",
              "IPY_MODEL_73e15358fbc34623977471e113035b8f"
            ],
            "layout": "IPY_MODEL_38bfcfc2b4fc4972a20b5d1d45317243"
          }
        },
        "80c47bb21c02403ab01e1a87343f52b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_831ca81d5e28459d8158e1fcd16cf112",
            "placeholder": "​",
            "style": "IPY_MODEL_421525a424424b6e8cabc4c2124339c5",
            "value": " 466k/466k [00:00&lt;00:00, 2.26MB/s]"
          }
        },
        "831ca81d5e28459d8158e1fcd16cf112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a58a3dded7d4649a53dc4c02b426cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9196e60b61954fb78571c123e780a4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada2275d1c4f43ad9c04e6f702ce8ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b799caea4b8c434ebbb86886a5fa7dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc26fb52ff7f400291cc9fdcf9f49e86",
              "IPY_MODEL_f84eb759ba124713b2736fa6e4c308e6"
            ],
            "layout": "IPY_MODEL_bc1eaff9c68e4e98bbd3a373dc1f4e0b"
          }
        },
        "ba5e219b14cf429faf2e6f97b811b327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4edfa42ca0214af4afa918898465d939",
              "IPY_MODEL_80c47bb21c02403ab01e1a87343f52b6"
            ],
            "layout": "IPY_MODEL_2c0901642f2745a7bc6da433de120c8e"
          }
        },
        "bc1eaff9c68e4e98bbd3a373dc1f4e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc26fb52ff7f400291cc9fdcf9f49e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ada2275d1c4f43ad9c04e6f702ce8ebb",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d613ba2dfc84ec2bd83a834f8053ea0",
            "value": 231508
          }
        },
        "cea1b70019b1421a9c67285dcdcbbe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db1b7ef2a88b475c91f5fec00ef36c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "ee07da1b2452472d99491785bd44c66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f84eb759ba124713b2736fa6e4c308e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9196e60b61954fb78571c123e780a4bf",
            "placeholder": "​",
            "style": "IPY_MODEL_cea1b70019b1421a9c67285dcdcbbe4a",
            "value": " 232k/232k [00:02&lt;00:00, 99.2kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
