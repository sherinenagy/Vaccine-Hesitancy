{"cells":[{"cell_type":"markdown","metadata":{"id":"6bvlYaayYuCI"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6252,"status":"ok","timestamp":1702983377091,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"N2R7JQdHFGBz"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys\n","from imblearn.over_sampling import RandomOverSampler\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702983377091,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"IwmbSKjrFF_c"},"outputs":[],"source":["# train_path = \"trainnew.csv\"\n","train_df = pd.read_csv('trainingaugnew.csv', encoding='utf-8')\n","\n","val_path=\"valnew.csv\"\n","test_path = \"testnew.csv\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3876,"status":"ok","timestamp":1702983380964,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"qMlPQF13FF9P"},"outputs":[],"source":["# train_df = pd.read_csv(train_path)\n","val_df = pd.read_csv(val_path)\n","test_df = pd.read_csv(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"EFbdl2AUYuCO","outputId":"e4476f55-7276-4482-9c98-bd88de440080"},"outputs":[],"source":["\n","classes =['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","\n","def updatedf(dfold):\n","\n","    # Add new columns with initial value 0\n","    dfold = pd.concat([dfold, pd.DataFrame(0, index=dfold.index, columns=classes)], axis=1)\n","\n","    # Iterate over each row and update the corresponding column to 1 based on Label1, Label2, and Label3\n","    for index, row in dfold.iterrows():\n","        if row['Label1'] in classes:\n","            dfold.at[index, row['Label1']] = 1\n","        if row['Label2'] in classes:\n","            dfold.at[index, row['Label2']] = 1\n","        if row['Label3'] in classes:\n","            dfold.at[index, row['Label3']] = 1\n","\n","    # Print the updated DataFrame\n","    print(dfold)\n","    return dfold\n","\n","\n","# train_df=updatedf(train_df)\n","val_df=updatedf(val_df)\n","test_df=updatedf(test_df)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"XY8o-zpmFF1z"},"outputs":[],"source":["# dropping useless features/columns\n","# train_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n","val_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)\n","test_df.drop(labels=['ID','Label1','Label2', 'Label3'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702983381526,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"5nY-51AnIFd5","outputId":"fb0fe19a-444f-4ea5-a3f1-72c549e63094"},"outputs":[],"source":["train_df.columns"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702983381527,"user":{"displayName":"Sherine Nagy Saleh","userId":"05967680675254348457"},"user_tz":-120},"id":"nkRnczi6G0ck"},"outputs":[],"source":["# rearranging columns\n","train_df = train_df[['Tweet', 'unnecessary', 'mandatory', 'pharma', 'conspiracy',\n","       'political', 'country', 'rushed', 'ingredients', 'side-effect',\n","       'ineffective', 'religious', 'none']]"]},{"cell_type":"markdown","metadata":{"id":"BFz9iR6hYuCU"},"source":["# Text Preprocessing Light"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"KdctPoIyYuCU"},"outputs":[],"source":["# import re, string\n","# import emoji\n","# import nltk\n","# from nltk.corpus import stopwords\n","\n","\n","# from sklearn import preprocessing\n","# from imblearn.over_sampling import RandomOverSampler\n","# from sklearn.model_selection import train_test_split\n","# def simple_text_clean(x):\n","#     if isinstance(x, list):\n","#         # If it's a list, join the elements into a string\n","#         x = ' '.join(x)\n","#         print(x)\n","#     # first we lowercase everything\n","#     x = x.lower()\n","#     # remove unicode characters\n","#     x = x.encode('ascii', 'ignore').decode()\n","#     x = re.sub(r'https*\\S+', ' ', x)\n","#     x = re.sub(r'http*\\S+', ' ', x)\n","#     #  Replace @ and the following word\n","#     x = re.sub(r'@(\\S+)', '', x)\n","\n","#     # Replace # and the following word\n","#     x = re.sub(r'#(\\S+)', ' ', x)\n","#     x = re.sub(r'\\'\\w+', '', x)\n","#     x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n","#     x = re.sub(r'\\w*\\d+\\w*', '', x)\n","#     x = re.sub(r'\\s{2,}', ' ', x)\n","#     x = re.sub(r'\\s[^\\w\\s]\\s', '', x)\n","#     return x"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KJ7ObTSHYuCW","outputId":"ca81d51d-b5f6-4db0-9ebc-c1631197f15d"},"outputs":[],"source":["\n","# train_df['Tweet'] = train_df['Tweet'].apply(simple_text_clean)\n","# val_df['Tweet'] = val_df['Tweet'].apply(simple_text_clean)\n","# test_df['Tweet'] = test_df['Tweet'].apply(simple_text_clean)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import pandas as pd\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import multilabel_confusion_matrix\n","from sklearn import metrics\n","import torch.nn as nn\n","from torch.optim.lr_scheduler import StepLR\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from transformers import AdamW\n","from transformers import BertTokenizer, BertModel\n","import torch.nn as nn\n","import numpy as np\n","import shutil\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from early_stopping import EarlyStopping\n","target_list = ['unnecessary', 'mandatory', 'pharma', 'conspiracy', 'political', 'country', 'rushed', 'ingredients', 'side-effect', 'ineffective', 'religious', 'none']\n","# hyperparameters\n","MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 16\n","EPOCHS = 2\n","LEARNING_RATE = 1e-05\n","import torch.cuda\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(device)\n","!nvcc --version\n","torch.__version__\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = df['Tweet']\n","        self.targets = self.df[target_list].values\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index])\n","        }"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","\n","ckpt_path = \"curr_ckpt\"\n","best_model_path = \"best_model.pt\"\n","# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n","# tokenizer=DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def LetsAssess(modelTBA):\n","    id2label = {idx:label for idx, label in enumerate(target_list)}\n","    label2id = {label:idx for idx, label in enumerate(target_list)}\n","\n","    #####Pass on all tweets and find their labels using the trained_model\n","    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n","\n","    predicted_labels = []\n","    predicted_single_labels=[]\n","    predicted_values=np.zeros((test_df.shape[0],12))\n","    predicted_raw=np.zeros((test_df.shape[0],12))\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    modelTBA.to(device)\n","\n","    for i,text in enumerate(test_df['Tweet']):\n","        \n","        encodings = tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=MAX_LEN,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Move the encodings to the device\n","        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n","        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n","        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n","        # # Perform the forward pass\n","        with torch.no_grad():\n","            output = modelTBA(input_ids, attention_mask, token_type_ids)\n","        \n","        # Apply sigmoid + threshold\n","        sigmoid = torch.nn.Sigmoid()\n","        probs = sigmoid(output.squeeze().cpu())    \n","        predictions = np.zeros(probs.shape)\n","        predictions[np.where(probs >= 0.5)] = 1\n","        if(sum(predictions))==0:\n","            argmax_index = probs.argmax()\n","            predictions[argmax_index] = 1\n","\n","        predicted_values[i]=predictions\n","        predicted_raw[i]=output.squeeze().cpu()\n","\n","    y_true=test_df[['unnecessary','mandatory','pharma','conspiracy','political','country','rushed','ingredients','side-effect','ineffective','religious','none']].to_numpy()\n","    \n","    print(classification_report(y_true, predicted_values,target_names=target_list))\n","\n","    print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n","\n","    multilabel_confusion_matrix(y_true, predicted_values)\n","\n","    # Calculate Jaccard score for each sample individually\n","    sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n","\n","    # Calculate the average Jaccard score\n","    average_jaccard = np.mean(sample_jaccard_scores)\n","    print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n","    return predicted_values, predicted_raw\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","val_targets=[]\n","val_outputs=[]\n","\n","def train_model(n_epochs, training_loader, validation_loader, model, \n","                optimizer, checkpoint_path, best_model_path):\n","  valid_loss_min = np.Inf\n","  for epoch in range(1, n_epochs+1):\n","    train_loss = 0\n","    valid_loss = 0\n","\n","    model.train()\n","    print('############# Epoch {}: Training Start   #############'.format(epoch))\n","    for batch_idx, data in enumerate(tqdm(training_loader)):\n","        #print('yyy epoch', batch_idx)\n","        ids = data['input_ids'].to(device, dtype = torch.long)\n","        mask = data['attention_mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs, targets)\n","        #if batch_idx%5000==0:\n","         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #print('before loss data in training', loss.item(), train_loss)\n","        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","        #print('after loss data in training', loss.item(), train_loss)\n","    \n","    print('############# Epoch {}: Training End     #############'.format(epoch))\n","    \n","    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################    \n","    # validate the model #\n","    ######################\n"," \n","    model.eval()\n","   \n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(tqdm(validation_loader, 0)):\n","            ids = data['input_ids'].to(device, dtype = torch.long)\n","            mask = data['attention_mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","\n","            loss = loss_fn(outputs, targets)\n","            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","            val_targets.extend(targets.cpu().detach().numpy().tolist())\n","            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      print('before cal avg train loss', train_loss)\n","      train_loss = train_loss/len(training_loader)\n","      valid_loss = valid_loss/len(validation_loader)\n","      # print training/validation statistics \n","      print('Epoch: {} \\tAverage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","      \n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","      # early_stopping(valid_loss, model)\n","        \n","       \n","        # save checkpoint\n","      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","      LetsAssess(model)\n","      # decode(predicted_values, predicted_raw )\n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","        # save checkpoint as best model\n","      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","        valid_loss_min = valid_loss\n","      # if early_stopping.early_stop:\n","      #       print(\"Early stopping\")\n","      #       break \n","    # scheduler.step()\n","    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","  # model.load_state_dict(torch.load(checkpoint_path))\n","  return model"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n","tokenizer=BertTokenizer.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n","\n","# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n","train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n","test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n","train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self, num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout = nn.Dropout(0.3)\n","        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n","\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0][:, 0, :]\n","        x = self.dropout(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesA,predicted_rawA=LetsAssess(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self, num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('seantw/covid-19-vaccination-tweet-stance')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout1 = nn.Dropout(0.3)\n","        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n","        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0]\n","        x = self.dropout1(x)\n","        x = self.transformer(x)\n","        x = self.dropout2(x)\n","        x = x.transpose(1, 2)  # Transpose the last two dimensions\n","        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n","        x = torch.cat(parallel_outputs, dim=1)\n","        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n","        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n","        x = self.dropout3(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesAA,predicted_rawAA=LetsAssess(model)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n","tokenizer=BertTokenizer.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n","\n","# tokenizer = BertTokenizer.from_pretrained('CovRelex-SE/CORD19-BERT')\n","train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n","test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n","train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout = nn.Dropout(0.3)\n","        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n","\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0][:, 0, :]\n","        x = self.dropout(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesB,predicted_rawB=LetsAssess(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('digitalepidemiologylab/covid-twitter-bert')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout1 = nn.Dropout(0.3)\n","        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n","        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0]\n","        x = self.dropout1(x)\n","        x = self.transformer(x)\n","        x = self.dropout2(x)\n","        x = x.transpose(1, 2)  # Transpose the last two dimensions\n","        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n","        x = torch.cat(parallel_outputs, dim=1)\n","        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n","        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n","        x = self.dropout3(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesBB,predicted_rawBB=LetsAssess(model)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n","test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n","train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-large-uncased')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout = nn.Dropout(0.3)\n","        self.clf = nn.Linear(self.hidden_size, 12)  # Adjust the input size of the final layer\n","\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0][:, 0, :]\n","        x = self.dropout(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesC,predicted_rawC=LetsAssess(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-large-uncased')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout1 = nn.Dropout(0.3)\n","        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.hidden_size, nhead=4), num_layers=1)\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n","        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0]\n","        x = self.dropout1(x)\n","        x = self.transformer(x)\n","        x = self.dropout2(x)\n","        x = x.transpose(1, 2)  # Transpose the last two dimensions\n","        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n","        x = torch.cat(parallel_outputs, dim=1)\n","        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n","        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n","        x = self.dropout3(x)\n","        x = self.clf(x)\n","        return x\n","\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesCC,predicted_rawCC=LetsAssess(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TransModel(nn.Module):\n","    def __init__(self,  num_layers=4, output_sizes=[16, 32, 64, 128]):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-large-uncased')\n","        self.hidden_size = self.bert.config.hidden_size\n","        self.dropout1 = nn.Dropout(0.3)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size, num_layers=1, batch_first=True)\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.parallel_layers = torch.nn.ModuleList([torch.nn.Conv1d(self.hidden_size, output_size, kernel_size=5, stride=4) for output_size in output_sizes])\n","        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n","        self.dense1 = nn.Linear(sum(output_sizes)*15, 256)  # Adjust the input size of the first dense layer\n","        self.dropout3 = nn.Dropout(0.3)\n","        self.clf = nn.Linear(256, 12)  # Adjust the input size of the final layer\n","\n","    def forward(self, inputs, mask, labels):\n","        cls_hs = self.bert(input_ids=inputs, attention_mask=mask, return_dict=False)\n","        x = cls_hs[0]\n","        x = self.dropout1(x)\n","        x, _ = self.gru(x)\n","        x = self.dropout2(x)\n","        x = x.transpose(1, 2)  # Transpose the last two dimensions\n","        parallel_outputs = [self.maxpool(layer(x)).squeeze(2) for layer in self.parallel_layers]\n","        x = torch.cat(parallel_outputs, dim=1)\n","        x = x.view(x.size(0), -1)  # Flatten the last two dimensions\n","        x = F.relu(self.dense1(x))  # Apply ReLU activation function after the first dense layer\n","        x = self.dropout3(x)\n","        x = self.clf(x)\n","        return x\n","best_model_path = \"modelA.pt\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = TransModel()\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, eps=1e-6)\n","\n","model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n","predicted_valuesD,predicted_rawD=LetsAssess(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","predicted_values=np.zeros((test_df.shape[0],12))\n","# pv=(predicted_rawB+predicted_rawD+predicted_rawC+predicted_rawF+predicted_rawG)/6#+predicted_rawH)/7\n","# pv=(predicted_rawC+predicted_rawD)/2\n","# pv=(predicted_rawB+predicted_rawC+predicted_rawD+predicted_rawE+predicted_rawF)/5#+predicted_rawH)/7\n","\n","pv=( predicted_rawA+predicted_rawB+predicted_rawC)/3\n","# pv=predicted_rawA\n","for i,text in enumerate(test_df['Tweet']):    \n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.from_numpy(pv[i]).to(device))\n","    predictions = np.zeros(probs.shape)\n","    predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n","    if(sum(predictions))==0:\n","         argmax_index = probs.argmax()\n","         predictions[argmax_index] = 1\n","    predicted_values[i]=predictions\n","\n","    \n","print(classification_report(y_true, predicted_values,target_names=target_list))\n","\n","print(\"Accuracy score\",accuracy_score(y_true, predicted_values))\n","\n","multilabel_confusion_matrix(y_true, predicted_values)\n","\n","# Calculate Jaccard score for each sample individually\n","sample_jaccard_scores = [metrics.jaccard_score(y_true[i], predicted_values[i]) for i in range(len(y_true))]\n","\n","# Calculate the average Jaccard score\n","average_jaccard = np.mean(sample_jaccard_scores)\n","print(\"Average Jaccard: {:.3f}\".format(average_jaccard))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from itertools import combinations\n","from sklearn.metrics import accuracy_score\n","import torch\n","import numpy as np\n","# 'A1':predicted_rawA,\n","# arrays = {'A1': predicted_rawA,'B1': predicted_rawB,'C1': predicted_rawC,'D1': predicted_rawAA,'E1': predicted_rawBB, 'F1':predicted_rawCC}#,'G1': predicted_rawG,'W1': predicted_rawW}#, 'H1':predicted_rawH, 'T1':predicted_rawT,'R1':predicted_rawR}\n","\n","arrays={'rawA':predicted_rawAloaded,'rawAD0':predicted_rawADOloaded,'rawAlr':predicted_rawAlrloaded,'rawW':predicted_rawWloaded}\n","\n","best_score = 0\n","best_combination = None\n","\n","for r in range(1, len(arrays) + 1):\n","    for combination in combinations(arrays.items(), r):\n","        # avg = sum(array for name, array in combination) / len(combination)\n","        avg = np.mean([array for name, array in combination], axis=0)\n","        sigmoid = torch.nn.Sigmoid()\n","        probs = sigmoid(torch.from_numpy(avg).to(device))\n","        predictions = np.zeros(probs.shape)\n","        predictions[np.where(probs.cpu().numpy() >= 0.5)] = 1    \n","        for i in range(predictions.shape[0]):\n","            if np.all(predictions[i] == 0):\n","                argmax_index = probs[i].argmax()\n","                predictions[i][argmax_index] = 1\n","        \n","        report = classification_report(y_true, predicted_values, output_dict=True)\n","        # score = report['macro avg']['f1-score']  # Replace with your preferred metric\n","        score = accuracy_score(y_true, predictions)\n","        if score > best_score:\n","            best_score = score\n","            best_combination = [name for name, array in combination]\n","\n","        # print(\"Combination:\", [name for name, array in combination])\n","\n","print(\"Best score:\", best_score)\n","print(\"Best combination:\", best_combination)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["tdn0Bhs-YuCR","BFz9iR6hYuCU","WipbuCXBYuCX"],"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
